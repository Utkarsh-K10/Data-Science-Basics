# -*- coding: utf-8 -*-
"""Major_project_progress.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OvwB-ByDaciBQkWk9dsaDZELoUtgjOwN

## Importing Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/Minor_project/cnn_images_dataset')

# from keras.models import Sequential, load_model
# from keras.layers import Convolution2D
# from keras.layers import MaxPooling2D
# from keras.layers import BatchNormalization, Dropout, Activation, Flatten, Dense
# from keras.applications.vgg16 import preprocess_input
# import matplotlib.pyplot as plt
# from numpy import expand_dims
# import numpy as np
# from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
# import pickle

"""# Part-1 CNN with softmax vs Sigmoid

## Prepearing Data set
"""

# Import os
# Import shutil
# os.chdir(“minor_project/cnn_image_dataset”)
# os.mkdir(‘train_set’)
# os.mkdir(‘test_set’)
# os.chdir(“minor_project/raw_dataset”)
# Files = os.listdir()

# For i in range(len(File)):
#      If file isnot None:
# for file in Files:
# 		If file.split(.)[0] == ’cat’:
# 			If os.path.exists(“minor_project/dataset/train’’):
# 				shutil.move(file,”minor_project/dataset/train/cat’’)
# 		else :
# 			If os.path.exists(“minor_project/dataset/train’’):
# 				shutil.move(file,”minor_project/dataset/train/dog’’)
# 	If i >8000:
# 		os.chdir(“minor_project/dataset/test’’)
# 		If file.split(.)[0] == ’cat’:
# 			If os.path.exists(“minor_project/dataset/test’’):
# 				shutil.move(file,”minor_project/dataset/test/cat’’)
# 		else :
# 			If os.path.exists(“minor_project/dataset/test’’):
# 				shutil.move(file,”minor_project/dataset/test/dog’’)
#    Else:
# print(“File format should be .png, .jpg, .jpeg”)

"""Creating Network Architecture

1. Now Defining Single Layered Network
"""

#initializing the CNN
classifier = Sequential()

#Step-1 convolution
classifier.add(Convolution2D(32,(3,3), input_shape = (64,64,3), activation = 'relu'))

#step-2 maxpooling
classifier.add(MaxPooling2D(pool_size =(2,2)))

#step-3 flattening
classifier.add(Flatten())

#step-4 full connection
classifier.add(Dense(units = 128, activation = 'relu'))#units = output_dim same
classifier.add(Dropout(0.13))
classifier.add(Dense(units = 1, activation ='sigmoid'))

#compile the CNN
classifier.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy', 'mse'])

#fitting CNN to th images(Training and Testing )
classifier.summary()

"""Preparing Data for training"""

train_datagen = ImageDataGenerator(rescale = 1./255, 
                                   rotation_range = 30,
                                   shear_range=0.2,
                                   zoom_range = 0.2,
                                   height_shift_range = 0.2,
                                   width_shift_range= 0.2, horizontal_flip =True, fill_mode = 'nearest')
training_data = train_datagen.flow_from_directory('training_set', batch_size = 32, target_size=(64,64), class_mode= 'binary')

test_datagen = ImageDataGenerator(rescale= 1./255)
testing_data = test_datagen.flow_from_directory('test_set', batch_size = 32, target_size = (64,64), class_mode = 'binary')

"""Fiting The Model"""

hist = classifier.fit_generator(training_data, steps_per_epoch = int(8006/32), epochs = 30, validation_data=testing_data, validation_steps= int(2013/32))
#epochs =125

"""Saving the trained model and History"""

os.chdir("/content/drive/My Drive/Minor_project/mode_history")

classifier.save("OC_model_1_sigmoid_30epoch.h5")

f = open("history_1_sigmoid_30epoch.pckl","wb")
pickle.dump(hist.history,f)
f.close()

"""Plotting Results"""

plt.plot(hist.history['accuracy'], label = "Accuracy")
plt.plot(hist.history['loss'], label = "Loss")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
# plt.plot(hist.history['mse'])

plt.plot(hist.history['val_accuracy'], label = "Val_Acc")
plt.plot(hist.history['val_loss'], label = "Val_Loss")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")
plt.title("Validation Data Result")
plt.legend()
plt.show()

plt.plot(hist.history['mse'], label = 'mse')
plt.plot(hist.history['val_mse'], label = 'val_mse')
plt.xlabel("Epochs")
plt.ylabel("Mean Squared Error")
plt.legend()
plt.show()

import os
os.chdir('/content/drive/My Drive/Minor_project/cnn_images_dataset')

"""2. Now Defining Network Layers
here we are creating 3-Layered(Hidden) CNN network
"""

Mlt_lyr_clfr = Sequential()

#First layer
Mlt_lyr_clfr.add(Convolution2D(32,(3,3), input_shape = (64,64,3), activation = 'relu'))
Mlt_lyr_clfr.add(BatchNormalization())
Mlt_lyr_clfr.add(MaxPooling2D(pool_size=(2,2)))
Mlt_lyr_clfr.add(Dropout(0.25))

#2nd Hidden Layer
Mlt_lyr_clfr.add(Convolution2D(64,(3,3), activation='relu'))
Mlt_lyr_clfr.add(BatchNormalization())
Mlt_lyr_clfr.add(MaxPooling2D(pool_size=(2,2)))
Mlt_lyr_clfr.add(Dropout(0.25))

#3rd Hidden layer
Mlt_lyr_clfr.add(Convolution2D(128, (3,3), activation='relu'))
Mlt_lyr_clfr.add(BatchNormalization())
Mlt_lyr_clfr.add(MaxPooling2D(pool_size=(2,2)))
Mlt_lyr_clfr.add(Dropout(0.25))

#4th output layer

Mlt_lyr_clfr.add(Flatten())
Mlt_lyr_clfr.add(Dense(units=512, activation='relu'))
Mlt_lyr_clfr.add(BatchNormalization())
# Mlt_lyr_clfr.add(Dense(units=1, activation= 'sigmoid'))
Mlt_lyr_clfr.add(Dense(units= 1, activation = 'softmax'))

Mlt_lyr_clfr.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy', 'mse'])
Mlt_lyr_clfr.summary()

train_datagen = ImageDataGenerator(rescale = 1./255, 
                                   rotation_range = 30,
                                   shear_range=0.2,
                                   zoom_range = 0.2,
                                   height_shift_range = 0.2,
                                   width_shift_range= 0.2, horizontal_flip =True, fill_mode = 'nearest')
training_data = train_datagen.flow_from_directory('training_set', batch_size = 32, target_size=(64,64), class_mode= 'binary')

test_datagen = ImageDataGenerator(rescale= 1./255)
testing_data = test_datagen.flow_from_directory('test_set', batch_size = 32, target_size = (64,64), class_mode = 'binary')

hist = Mlt_lyr_clfr.fit_generator(training_data, steps_per_epoch = int(8006/32),epochs = 30, validation_data = testing_data, validation_steps = int(2013/32))

os.chdir("/content/drive/My Drive/Minor_project/mode_history")

Mlt_lyr_clfr.save("Mlti_lyr_mdl_Sftmx_30_epch.h5")

f = open("hstry_mlt_lyr_mdl_Sftmx_30_epch.pckl", "wb")
pickle.dump(hist.history, f)
f.close()

plt.plot(hist.history['accuracy'], label = "Accuracy")
plt.plot(hist.history['loss'], label = "Loss")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

plt.plot(hist.history['val_accuracy'], label = "Val_Acc")
plt.plot(hist.history['val_loss'], label = "Val_Loss")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")
plt.title("Validation Data Result")
plt.legend()
plt.show()

plt.plot(hist.history['mse'], label = 'mse')
plt.plot(hist.history['val_mse'], label = 'val_mse')
plt.xlabel("Epochs")
plt.ylabel("Mean Squared Error")
plt.legend()

os.chdir('/content/drive/My Drive/Minor_project/mode_history')
import pickle

f = open("history_1_sigmoid_30epoch.pckl","rb")
hs1 = pickle.load(f)
f.close()

import matplotlib.pyplot as plt
plt.plot(hs1['accuracy'], label = "Accuracy")
plt.plot(hs1['val_accuracy'], label= "Val Accuracy")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")
plt.title("1-layerd CNN with Sigmoid ")
plt.legend()

plt.plot(hs1['loss'], label = "Loss")
plt.plot(hs1['val_loss'], label = "Val Loss")
plt.xlabel("No of Epochs")
plt.ylabel("Loss")
plt.legend()

f2 = open('hstry_mlt_lyr_mdl_Sigmoid_30_epch.pckl','rb')
hs2 = pickle.load(f2)
f2.close()

plt.plot(hs2['accuracy'], label= "Accuracy" )
plt.plot(hs2['val_accuracy'], label = "Val_accuracy")
plt.legend()
plt.title("3-layerd CNN with Sigmoid ")
plt.xlabel("No of Epochs")
plt.ylabel("Accuracy")

plt.plot(hs2['loss'], label = "Loss")
plt.plot(hs2['val_loss'], label = "Validation Loss")
plt.legend()
plt.title("CNN with Sigmoid ")
plt.xlabel("No of Epochs")
plt.ylabel("Loss")

madal = load_model('/content/drive/My Drive/Minor_project/mode_history/Mlti_lyr_mdl_Sigmd_30_epch.h5')

madal.evaluate(testing_data)

# 63/63 [==============================] - 1498s 24s/step - loss: 0.4528 - accuracy: 0.7889 - mean_squared_error: 0.1464
# [0.45283037424087524, 0.7888723015785217, 0.14638696610927582]

"""# Part -2 with CNN-SVM/RF  combined Model

####Preparing Data
"""

import os 
import cv2
import numpy as np

# os.chdir("/content/drive/My Drive/Minor_project/cnn_images_dataset/training_set")
# os.listdir()

# SIZE = 256
# dir = '/content/drive/My Drive/Minor_project/cnn_images_dataset/training_set'
# categories = ["dogs", "cats"]
# train_images = []
# train_labels =[]

# for category in categories:
#   dir_path = os.path.join(dir,category)
#   label = categories.index(category)
#   print(label)
#   for img in os.listdir(dir_path):
#     img_path = os.path.join(dir_path,img)
#     img = cv2.imread(img_path, cv2.IMREAD_COLOR)
#     try:
#       img = cv2.resize(img, (SIZE,SIZE))
#       img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
#       train_images.append(img)
#       train_labels.append(label)
#     except Exception as e:
#       pass

# train_images = np.array(train_images)
# train_labels = np.array(train_labels)
# print("Done")

"""#### Dumping the saved data """

# import pickle
# trn_img = open("train_img_np_gghjhffdrdata.pickle", "wb")
# pickle.dump(train_images,trn_img)
# trn_img.close()

# import pickle
# trn_lbl = open("train_lbl_hghghjgnp_data.pickle", "wb")
# pickle.dump(train_labels,trn_lbl)
# trn_lbl.close()

SIZE = 256
import os
os.chdir("/content/drive/My Drive/Minor_project/cnn_images_dataset")
os.listdir()
import pickle

tst_img = open('test_img_np_data.pickle', 'rb')
test_images = pickle.load(tst_img)
tst_img.close()

tst_lbl = open('test_lbl_np_data.pickle', 'rb')
test_labels = pickle.load(tst_lbl)
tst_lbl.close()

trn_img = open('train_img_np_data.pickle', 'rb')
train_images = pickle.load(trn_img)
trn_img.close()

trn_lbl = open('train_lbl_np_data.pickle', 'rb')
train_labels = pickle.load(trn_lbl)
trn_lbl.close()

import random
mapped = list(zip(train_images,train_labels))
random.shuffle(mapped)
train_images, train_labels = zip(*mapped)
train_images = np.array(train_images)
train_labels = np.array(train_labels)
del(mapped)

mapped = list(zip(test_images,test_labels))
random.shuffle(mapped)
test_images,test_labels = zip(*mapped)
test_images = np.array(test_images)
test_labels = np.array(test_labels)
del(mapped)

# len(test_images[0])

"""#### Spliting the data"""

x_train, y_train, x_test, y_test  = train_images[0:2500], train_labels[0:2500], test_images[0:875], test_labels[0:875]

del(test_images)
del(train_images)
# del(test_labels)
del(train_labels)

x_train , x_test = x_train/255.0, x_test/255.0

"""#### Ectracting features Using VGG16"""

from keras.applications.vgg16 import VGG16

SIZE = 256
VGG_model = VGG16(weights = 'imagenet', include_top =False, input_shape = (SIZE,SIZE,3))

for layer in VGG_model.layers:
  layer.trainable = False
VGG_model.summary()
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

x_train_features= VGG_model.predict(x_train) 
rf_tr_features = x_train_features.reshape(x_train_features.shape[0],-1)
X_tr_RF = rf_tr_features

del(x_train)

"""### SVM"""

import matplotlib.pyplot as plt
plt.scatter(X_tr_RF[0:100],X_ts_RF[0:100])

from sklearn.svm import SVC
svm_model = SVC(kernel='linear',gamma=0.001, random_state=7)
svm_model.fit(X_tr_RF, y_train[0:2500])

# f = open("svm_model.pickle","wb")
# pickle.dump(svm_model,f)
# f.close()

prediction_svm = svm_model.predict(X_ts_RF)



from sklearn import metrics 
from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt


mse = metrics.mean_squared_error(prediction_svm,test_labels[0:875])
print("Accuracy", metrics.accuracy_score(prediction_svm,test_labels[0:875]),mse)
cm = confusion_matrix(test_labels[0:875], prediction_svm)
sns.heatmap(cm,annot=True)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier
RF_model = RandomForestClassifier(n_estimators=50, random_state = 82)
RF_model.fit(X_tr_RF, y_train[0:2500])

x_test_features = VGG_model.predict(x_test[0:500])
rf_ts_features = x_test_features.reshape(x_test_features.shape[0],-1)
X_ts_RF = rf_ts_features

predeiction_RF = RF_model.predict(X_ts_RF)

del(X_ts_RF)
del(X_tr_RF)
del(x_test_features)
del(x_train_features)
del(rf_ts_features)
del(rf_tr_features)

from sklearn import metrics 
from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

mse = metrics.mean_squared_error(predeiction_RF,test_labels[0:500])

print("Accuracy", metrics.accuracy_score(predeiction_RF,test_labels[0:500]))
print(metrics.classification_report(predeiction_RF,test_labels[0:500]))

# Accuracy 0.158(50,42)  0.146(100, 82)  Accuracy 0.16(50, 82) mse =0.84
#Accuracy 0.082 svm with linear
#  svm with 0.082 linear & gamma 0.001
# Accuracy 0.102 svm wth rbf
# Accuracy 0.138 svm wth rbf & gamma 0.001
#               precision    recall  f1-score   support

#            0       0.18      0.30      0.23       203
#            1       0.12      0.06      0.08       297

#     accuracy                           0.16       500
#    macro avg       0.15      0.18      0.15       500
# weighted avg       0.14      0.16      0.14       500

cm = confusion_matrix(test_labels[0:500], predeiction_RF)
sns.heatmap(cm,annot=True)

# f = open("RF.pickle","wb")
# pickle.dump(predeiction_RF,f)
# f.close()

"""###Testing Real Data """

n = np.random.randint(0,x_test[0:500].shape[0])
img = x_test[0:500][n]
plt.imshow(img)
input_img = np.expand_dims(img,axis = 0)
inp_img_featr = VGG_model.predict(input_img)
input_img_features = inp_img_featr.reshape(inp_img_featr.shape[0],-1)

test_img_RF = input_img_features 
# result = RF_model.predict(test_img_RF)
result = svm_model.predict(test_img_RF)
if result == 1:
  print("Cat",result)
else:
  print('Dog',result)

import pandas as pd
import numpy as np
import pickle
import os
os.chdir("/content/drive/My Drive/Minor_project/mode_history")

Accuracy 0.134
              precision    recall  f1-score   support

           0       0.15      0.24      0.18       199
           1       0.11      0.06      0.08       301

    accuracy                           0.13       500
   macro avg       0.13      0.15      0.13       500
weighted avg       0.13      0.13      0.12       500

data =[['0',0.15,0.24,0.18,199],['1', 0.11,0.06,0.08,301],['accuracy',0,0,0.13,500]]

report = pd.DataFrame(data, columns=['Labels','precision','recall','f1-score','support'])

report

report.to_csv("reprot_RF_data.csv")

report.drop(columns=['Labels'], axis = 1)

vgg_summry = []

_________________________________________________________________
Layer (type)                 Output Shape              Param    
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

